# Impact Measurement Plan

The North African AI Safety Lab uses a mixed-methods approach to assess the real-world impact of our tools on youth safety and educational outcomes.

## 1. Quantitative Indicators (Telemetry)

We collect privacy-preserving telemetry to track:

*   **Intervention Volume:** Number of unsafe interactions blocked by the agent.
*   **Session Quality:** Duration of safe, uninterrupted educational sessions.
*   **Latency Impact:** The added time cost of the "Verify-then-Reply" layer (target: < 1.5s).

## 2. Qualitative Feedback (Wellbeing)

We incorporate optional, non-clinical feedback mechanisms:

*   **Mentor Debriefs:** Weekly structured interviews with adult supervisors to identify behavioral trends.
*   **Participant Sentiment:** Simple "thumbs up/down" or emoji-based feedback on the helpfulness of the AI tutor.

## 3. Blinded Audits

To ensure our safety definitions remain culturally relevant:

1.  **Selection:** We sample 50 interactions per week (anonymised).
2.  **Review:** Two independent reviewers (cultural experts) label the interactions for safety and helpfulness.
3.  **Consensus:** Disagreements are resolved by a third senior reviewer.
4.  **Metric:** We track the "Agreement Rate" between the AI system's decision and the human experts.

## 4. Long-Term Outcomes

We aim to measure:

*   **Trust:** Do educators trust the tool more after seeing the safety layer in action?
*   **Adoption:** Number of classrooms/NGOs deploying the toolkit.
*   **Policy Influence:** Citations of our work in regional AI safety guidelines.

## 5. Publishing Results

We commit to publishing an annual "State of Safety" report summarizing these metrics across all deployments, ensuring transparency while protecting individual partner identities.
